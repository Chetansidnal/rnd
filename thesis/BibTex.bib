@article{bengio2003nlp,
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Janvin, Christian},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
pages = {1137--1155},
publisher = {JMLR.org},
title = {{A Neural Probabilistic Language Model}},
url = {http://dl.acm.org/citation.cfm?id=944919.944966},
volume = {3},
year = {2003}
}
@article{champandard2016semanticstyle,
abstract = {Convolutional neural networks (CNNs) have proven highly effective at image synthesis and style transfer. For most users, however, using them as tools can be a challenging task due to their unpredictable behavior that goes against common intuitions. This paper introduces a novel concept to augment such generative architectures with semantic annotations, either by manually authoring pixel labels or using existing solutions for semantic segmentation. The result is a content-aware generative algorithm that offers meaningful control over the outcome. Thus, we increase the quality of images generated by avoiding common glitches, make the results look significantly more plausible, and extend the functional range of these algorithms---whether for portraits or landscapes, etc. Applications include semantic style transfer and turning doodles with few colors into masterful paintings!},
archivePrefix = {arXiv},
arxivId = {1603.01768},
author = {Champandard, Alex J.},
eprint = {1603.01768},
file = {:home/nitred/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Champandard - 2016 - Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks.pdf:pdf},
journal = {CoRR},
title = {{Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks}},
url = {http://arxiv.org/abs/1603.01768},
year = {2016}
}
@misc{doersch2016vae,
abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of
the most popular approaches to unsupervised learning of complicated
distributions. VAEs are appealing because they are built on top of standard
function approximators (neural networks), and can be trained with stochastic
gradient descent. VAEs have already shown promise in generating many kinds of
complicated data, including handwritten digits, faces, house numbers, CIFAR
images, physical models of scenes, segmentation, and predicting the future from
static images. This tutorial introduces the intuitions behind VAEs, explains
the mathematics behind them, and describes some empirical behavior. No prior
knowledge of variational Bayesian methods is assumed.},
annote = {cite arxiv:1606.05908},
author = {Doersch, Carl},
keywords = {VAE sgd},
title = {{Tutorial on Variational Autoencoders}},
url = {http://arxiv.org/abs/1606.05908},
year = {2016}
}
@article{gatys2015artistic,
abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the con- tent and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. How- ever, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks.1,2 Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to sepa- rate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the strik- ing similarities between performance-optimised artificial neural networks and biological vision,3–7 our work offers a path forward to an algorithmic under- standing of how humans create and perceive artistic imagery.},
archivePrefix = {arXiv},
arxivId = {1508.06576},
author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
doi = {10.1561/2200000006},
eprint = {1508.06576},
file = {:home/nitred/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gatys, Ecker, Bethge - 2015 - A Neural Algorithm of Artistic Style.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
journal = {arXiv preprint},
keywords = {eural algorithm of artistic,style},
pages = {3--7},
pmid = {1000200972},
title = {{A Neural Algorithm of Artistic Style}},
year = {2015}
}
@article{goodfellow2014gan,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {1406.2661},
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
doi = {10.1001/jamainternmed.2016.8245},
eprint = {1406.2661},
file = {:home/nitred/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodfellow et al. - 2014 - Generative Adversarial Nets.pdf:pdf},
isbn = {1406.2661},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 27},
pages = {2672--2680},
pmid = {15040217},
title = {{Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1406.2661},
year = {2014}
}
@inproceedings{kavucuoglu2009multistage,
abstract = {In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63{\%} recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6{\%}) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 ({\&}gt; 65{\%}), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53{\%}).},
author = {Jarrett, Kevin and Kavukcuoglu, Koray and Ranzato, Marc'Aurelio and LeCun, Yann},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2009.5459469},
file = {:home/nitred/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jarrett et al. - 2009 - What is the best multi-stage architecture for object recognition.pdf:pdf},
isbn = {9781424444205},
issn = {1550-5499},
pages = {2146--2153},
title = {{What is the best multi-stage architecture for object recognition?}},
year = {2009}
}
@article{kavukcuoglu2010hierarchies,
abstract = {We propose an unsupervisedmethod for learningmulti-stage hierarchies of sparse convolutional features. While sparse coding has become an increasingly popular method for learning visual features, it is most often trained at the patch level. Applying the resulting filters convolutionally results in highly redundant codes because overlapping patches are encoded in isolation. By training convolutionally over large image windows, our method reduces the redudancy between feature vectors at neighboring locations and improves the efficiency of the overall repre- sentation. In addition to a linear decoder that reconstructs the image from sparse features, our method trains an efficient feed-forward encoder that predicts quasi- sparse features from the input. While patch-based training rarely produces any- thing but oriented edge detectors, we show that convolutional training produces highly diverse filters, including center-surround filters, corner detectors, cross de- tectors, and oriented grating detectors. We show that using these filters in multi- stage convolutional network architecture improves performance on a number of visual recognition and detection tasks.},
author = {Kavukcuoglu, Koray and Sermanet, Pierre and Boureau, Y-Lan and Gregor, Karol and Mathieu, Michael and LeCun, Yann},
file = {:home/nitred/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kavukcuoglu et al. - 2010 - Learning Convolutional Feature Hierarchies for Visual Recognition.pdf:pdf},
isbn = {9781617823800},
issn = {-},
journal = {Nips},
number = {1},
pages = {1090--1098},
title = {{Learning Convolutional Feature Hierarchies for Visual Recognition}},
year = {2010}
}
@article{kingma2014adam,
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P and Ba, Jimmy},
eprint = {1412.6980},
journal = {CoRR},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
volume = {abs/1412.6},
year = {2014}
}
@article{kingma2013vae,
author = {Kingma, Diederik P and Welling, Max},
journal = {CoRR},
keywords = {dblp},
title = {{Auto-Encoding Variational Bayes.}},
url = {http://dblp.uni-trier.de/db/journals/corr/corr1312.html{\#}KingmaW13},
volume = {abs/1312.6},
year = {2013}
}
@article{kiros2015skip,
author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
journal = {arXiv preprint arXiv:1506.06726},
title = {{Skip-Thought Vectors}},
year = {2015}
}
@article{krizhevsky2012alexnet,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:home/nitred/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@inproceedings{lecun1998mnist,
author = {Lecun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
booktitle = {Proceedings of the IEEE},
pages = {2278--2324},
title = {{Gradient-based learning applied to document recognition}},
year = {1998}
}
@article{lin2014mscoco,
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge J and Bourdev, Lubomir D and Girshick, Ross B and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C Lawrence},
eprint = {1405.0312},
journal = {CoRR},
title = {{Microsoft COCO: Common Objects in Context}},
url = {http://arxiv.org/abs/1405.0312},
volume = {abs/1405.0},
year = {2014}
}
@article{mikolov2013word2vec1,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
isbn = {1532-4435},
issn = {15324435},
journal = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/pdf/1301.3781v3.pdf},
year = {2013}
}
@incollection{mikolov2013word2vec2,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {Advances in Neural Information Processing Systems 26},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
isbn = {2150-8097},
issn = {10495258},
pages = {3111--3119},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {http://arxiv.org/abs/1310.4546},
year = {2013}
}
@inproceedings{mikolov2013wordrep,
abstract = {

Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40{\%} of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.


},
author = {Mikolov, Tomas and Yih, Scott Wen-tau and Zweig, Geoffrey},
booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2013)},
publisher = {Association for Computational Linguistics},
title = {{Linguistic Regularities in Continuous Space Word Representations}},
url = {https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/},
year = {2013}
}
@article{mirza2014cgan,
abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
archivePrefix = {arXiv},
arxivId = {1411.1784},
author = {Mirza, Mehdi and Osindero, Simon},
eprint = {1411.1784},
file = {:home/nitred/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mirza, Osindero - 2014 - Conditional Generative Adversarial Nets.pdf:pdf},
journal = {CoRR},
pages = {1--7},
title = {{Conditional Generative Adversarial Nets}},
url = {http://arxiv.org/abs/1411.1784},
year = {2014}
}
@inproceedings{nilsback2008flowers,
author = {Nilsback, M-E. and Zisserman, A},
booktitle = {Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing},
title = {{Automated Flower Classification over a Large Number of Classes}},
year = {2008}
}
@article{noh2015deconvseg,
author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
journal = {CoRR},
title = {{Learning Deconvolution Network for Semantic Segmentation}},
url = {http://arxiv.org/abs/1505.04366},
volume = {abs/1505.0},
year = {2015}
}
@article{radford2015dcgan,
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
file = {:home/nitred/Downloads/papers/Radford2015{\_}DCGAN.pdf:pdf},
journal = {CoRR},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
volume = {abs/1511.0},
year = {2015}
}
@inproceedings{reed2016texttoimage,
abstract = {Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neu-ral network architectures have been developed to learn discriminative text feature representa-tions. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to gen-erate highly compelling images of specific cat-egories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image model-ing, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.},
archivePrefix = {arXiv},
arxivId = {1605.05396},
author = {Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
booktitle = {ICML},
eprint = {1605.05396},
file = {:home/nitred/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Reed et al. - 2016 - Generative Adversarial Text to Image Synthesis.pdf:pdf},
pages = {1060--1069},
title = {{Generative Adversarial Text to Image Synthesis}},
year = {2016}
}
@article{imagenet,
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
journal = {International Journal of Computer Vision (IJCV)},
number = {3},
pages = {211--252},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@article{zisserman2014vgg,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:home/nitred/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
journal = {ImageNet Challenge},
pages = {1--10},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{oord2016pixelrnn,
author = {van den Oord, A{\"{a}}ron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
journal = {CoRR},
title = {{Pixel Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1601.06759},
volume = {abs/1601.0},
year = {2016}
}
@article{oord2016pixelcnn,
author = {van den Oord, A{\"{a}}ron and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
journal = {CoRR},
title = {{Conditional Image Generation with PixelCNN Decoders}},
url = {http://arxiv.org/abs/1606.05328},
volume = {abs/1606.0},
year = {2016}
}
@article{xu2015relus,
archivePrefix = {arXiv},
arxivId = {1505.00853},
author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
eprint = {1505.00853},
journal = {CoRR},
title = {{Empirical Evaluation of Rectified Activations in Convolutional Network}},
url = {http://arxiv.org/abs/1505.00853},
volume = {abs/1505.00853},
year = {2015}
}
@inproceedings{zeiler2014viscnn,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D. and Fergus, Rob},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
file = {:home/nitred/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeiler, Fergus - 2014 - Visualizing and understanding convolutional networks.pdf:pdf},
isbn = {9783319105895},
issn = {16113349},
number = {PART 1},
pages = {818--833},
pmid = {26353135},
title = {{Visualizing and understanding convolutional networks}},
volume = {8689 LNCS},
year = {2014}
}
@inproceedings{zeiler2010deconv,
abstract = {Building robust low and mid-level image representations, beyond edge primitives, is a long-standing goal in vision. Many existing feature detectors spatially pool edge information which destroys cues such as edge intersections, parallelism and symmetry. We present a learning framework where features that capture these mid-level cues spontaneously emerge from image data. Our approach is based on the convolutional decomposition of images under a spar-sity constraint and is totally unsupervised. By building a hierarchy of such decompositions we can learn rich feature sets that are a robust image representation for both the analysis and synthesis of images.},
archivePrefix = {arXiv},
arxivId = {1302.1700},
author = {Zeiler, Matthew D. and Krishnan, Dilip and Taylor, Graham W. and Fergus, Rob},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5539957},
eprint = {1302.1700},
file = {:home/nitred/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeiler et al. - 2010 - Deconvolutional networks.pdf:pdf},
isbn = {9781424469840},
issn = {10636919},
pages = {2528--2535},
pmid = {16190471},
title = {{Deconvolutional networks}},
year = {2010}
}
@article{zhang2016stackgan,
author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Huang, Xiaolei and Wang, Xiaogang and Metaxas, Dimitris N},
file = {:home/nitred/Downloads/papers/Zhang2016{\_}StackGAN.pdf:pdf},
journal = {CoRR},
title = {{StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1612.03242},
volume = {abs/1612.0},
year = {2016}
}
